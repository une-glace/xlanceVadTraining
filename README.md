# XVAD: 基于合成训练的轻量级声学 VAD

本项目实现了一个基于 **合成数据生成 (Synthetic Data Generation)** 训练的轻量级语音活动检测 (VAD) 模型。它在训练过程中动态混合纯净语音（如 WenetSpeech）和背景噪声（如 MUSAN），无需人工标注的时间戳。

## 1. 环境依赖

安装必要的 Python 库：

```bash
pip install torch torchaudio wandb numpy tqdm
```

## 2. 数据准备 (Data Preparation)

本项目使用 **WenetSpeech** (中文语音) 和 **MUSAN** (背景噪声) 进行合成训练。

### 关于数据集
*   **WenetSpeech**: 一个大规模的中文语音识别数据集。我们使用其训练集中的纯净语音部分。
*   **MUSAN**: 一个包含音乐、语音和噪声的数据集。在 VAD 训练中，我们主要使用其 `noise` 子集，其中包括：
    *   **FreeSound**: 来自 freesound.org 的环境噪音（如雨声、街道声）。
    *   **SoundBible**: 来自 soundbible.com 的特定音效（如门铃、动物叫声）。

### 数据预处理流程

为了提高训练效率，我们提供了一键式预处理脚本 `run_preprocessing.py`。该脚本会自动完成以下工作：
1.  **扫描**原始数据集路径。
2.  **重采样**所有音频到 16000Hz (VAD 标准采样率)。
3.  **转换**为单声道 (Mono)。
4.  **保存**到新的工作目录。
5.  **生成**索引列表 (`wav.scp`) 供训练使用。

**运行预处理：**

```bash
python run_preprocessing.py
```

*注意：脚本会自动检测 CPU 核数并开启多进程加速处理。*

## 3. 训练 (Training)

训练脚本 `train_vad.py` 会读取预处理生成的 `.scp` 文件，通过 `dataset.py` 执行在线混合 (On-the-fly Mixing)，并将指标记录到 [WandB](https://wandb.ai/)。

### 开始训练

#### 单卡训练 (Single GPU)
```bash
python train_vad.py --batch_size 128
```

#### 多卡训练 (Multi-GPU)
推荐使用 `torchrun` 启动（例如使用 4 张卡）：

```bash
# CUDA_VISIBLE_DEVICES: 指定具体的 GPU 编号
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 train_vad.py --batch_size 128
```

*注意：首次运行时，WandB 会要求输入你的 API Key。*

### 训练细节
*   **数据集**: `SyntheticVADDataset` (在 `dataset.py` 中) 动态混合语音和噪声，随机信噪比 (SNR) 为 5dB - 20dB。
*   **模型**: `XVADModel` (在 `model.py` 中) 是一个受 Silero VAD 启发的 CRNN (Conv1d + LSTM) 模型。
*   **日志**: Loss 和指标将记录到 WandB 项目 `xvad-training` 中。
*   **检查点**: 每个 epoch 结束后模型会保存到 `checkpoints/` 目录。

## 项目结构 (Project Structure)

*   **`run_preprocessing.py` (核心预处理)**: 
    *   负责数据清洗的“流水线”。它将原始 WenetSpeech/MUSAN 数据重采样到 16k，存入本地目录，并自动生成 `.scp` 索引文件。**项目启动的第一步。**
*   **`dataset.py` (数据加载与混合)**: 
    *   PyTorch Dataset 定义。它在训练过程中实时读取处理好的音频，进行“语音+噪声”的动态混合 (Mixing) 和标签生成。
*   **`prepare_list.py` (索引工具)**: 
    *   用于扫描目录并生成 `.scp` 文件列表的辅助工具。（`run_preprocessing.py` 已经包含了此功能，通常无需单独运行）。
*   **`train_vad.py` (训练主程序)**: 
    *   包含训练循环、WandB 日志记录、模型保存和 DDP 多卡支持。
*   **`model.py`**: 
    *   CRNN VAD 模型架构定义。

---

# XVAD: Lightweight Acoustic VAD with Synthetic Training

This project implements a lightweight Voice Activity Detection (VAD) model trained using **Synthetic Data Generation**. It dynamically mixes clean speech (e.g., WenetSpeech) with background noise (e.g., MUSAN) during training, eliminating the need for manually labeled timestamps.

## 1. Requirements

Install the necessary Python libraries:

```bash
pip install torch torchaudio wandb numpy tqdm
```

## 2. Data Preparation

This project uses **WenetSpeech** (Mandarin Speech) and **MUSAN** (Background Noise) for synthetic training.

### About Datasets
*   **WenetSpeech**: A large-scale Mandarin ASR corpus. We use the clean speech subset from its training set.
*   **MUSAN**: A corpus of Music, Speech, and Noise. For VAD training, we specifically use the `noise` subset, which includes:
    *   **FreeSound**: Environmental noises from freesound.org.
    *   **SoundBible**: Sound effects from soundbible.com.

### Preprocessing Pipeline

To improve training efficiency, we provide a one-stop preprocessing script `run_preprocessing.py`. This script automatically:
1.  **Scans** original dataset paths.
2.  **Resamples** all audio to 16000Hz (standard VAD sample rate).
3.  **Converts** audio to Mono.
4.  **Saves** processed files to a local directory.
5.  **Generates** index lists (`wav.scp`) for training.

**Run Preprocessing:**

```bash
python run_preprocessing.py
```

*Note: The script automatically detects CPU cores and enables multi-processing for speed.*

## 3. Training

The training script `train_vad.py` reads the `.scp` files generated by the preprocessor, performs **On-the-fly Mixing** via `dataset.py`, and logs metrics to [WandB](https://wandb.ai/).

### Start Training

#### Single GPU
```bash
python train_vad.py --batch_size 128
```

#### Multi-GPU
It is recommended to use `torchrun` (e.g., with 4 GPUs):

```bash
# CUDA_VISIBLE_DEVICES: Specify GPU IDs
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 train_vad.py --batch_size 128
```

*Note: On the first run, WandB will ask for your API key.*

### Training Details
*   **Dataset**: `SyntheticVADDataset` (in `dataset.py`) dynamically mixes speech and noise with random SNRs (5dB - 20dB).
*   **Model**: `XVADModel` (in `model.py`) is a CRNN (Conv1d + LSTM) inspired by Silero VAD.
*   **Logging**: Loss and metrics are logged to WandB project `xvad-training`.
*   **Checkpoints**: Models are saved to `checkpoints/` after every epoch.

## Project Structure

*   **`run_preprocessing.py` (Core Preprocessing)**: 
    *   The data cleaning pipeline. It resamples raw WenetSpeech/MUSAN data to 16k, saves them locally, and auto-generates `.scp` index files. **Run this first.**
*   **`dataset.py` (Loader & Mixing)**: 
    *   The PyTorch Dataset definition. It performs real-time mixing of speech and noise during training loops.
*   **`prepare_list.py` (Indexer)**: 
    *   Helper tool to scan directories and create `.scp` lists. (Functionality included in `run_preprocessing.py`, so usually not needed separately).
*   **`train_vad.py` (Trainer)**: 
    *   Main training loop with WandB integration and DDP support.
*   **`model.py`**: 
    *   The CRNN VAD model architecture.
